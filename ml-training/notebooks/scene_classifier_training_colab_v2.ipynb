{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üèÅ RaceTagger Scene Classifier Training v2 - ResNet18 Optimized\n",
    "\n",
    "**Obiettivo**: Replicare risultati Roboflow (89% accuracy) con training locale.\n",
    "\n",
    "## Ottimizzazioni Chiave:\n",
    "- ‚úÖ **ResNet18** (11.7M params) invece di ResNet50 (25.6M) - previene overfitting\n",
    "- ‚úÖ **MixUp** augmentation - specifico per classification\n",
    "- ‚úÖ **Label Smoothing 0.1** - previene overconfidence\n",
    "- ‚úÖ **Cosine Annealing LR** con warmup\n",
    "- ‚úÖ **Class Weights** ottimizzati per bilanciamento\n",
    "- ‚úÖ **Augmentation potenziata** (rotation ¬±25¬∞, brightness 0.7-1.3)\n",
    "\n",
    "## Setup\n",
    "1. Runtime ‚Üí Change runtime type ‚Üí **GPU (T4)**\n",
    "2. Dataset gi√† caricato su Google Drive\n",
    "3. Run cells sequentially\n",
    "\n",
    "**Target**: 85-91% validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "install"
   },
   "source": "# Check GPU availability\n!nvidia-smi\n\n# Install dependencies\n!pip install -q tensorflow pillow tensorflowjs matplotlib seaborn\n!pip install -q image-classifiers  # Pacchetto PyPI corretto per ResNet18\n\nimport tensorflow as tf\nprint(f\"\\n‚úÖ TensorFlow version: {tf.__version__}\")\nprint(f\"‚úÖ GPU available: {tf.config.list_physical_devices('GPU')}\")\n\n# Verify classification_models - import corretto\nfrom classification_models.keras import Classifiers\nprint(\"‚úÖ classification_models installed\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mount_drive"
   },
   "source": "# Mount Google Drive\nfrom google.colab import drive\nimport os\nfrom pathlib import Path\n\ndrive.mount('/content/drive')\n\nprint(\"\\nüîç Searching for dataset on Google Drive...\\n\")\n\n# Possible dataset locations on Drive\nPOSSIBLE_PATHS = [\n    '/content/drive/MyDrive/FP - Federico Pasinetti/Progetti Personali/RaceTagger/scene_classifier_ML/f1_scenes_dataset',\n    '/content/drive/MyDrive/f1_scenes_dataset',\n    '/content/drive/MyDrive/RaceTagger/f1_scenes_dataset',\n    '/content/drive/MyDrive/ml-training/f1_scenes_dataset',\n]\n\n# Find dataset\nDATASET_ROOT = None\nfor path in POSSIBLE_PATHS:\n    if os.path.exists(path):\n        DATASET_ROOT = path\n        print(f\"‚úÖ Dataset found: {path}\")\n        break\n\nif DATASET_ROOT is None:\n    print(\"‚ùå Dataset not found in common locations!\")\n    print(\"\\nüìÇ Available folders in MyDrive:\")\n    !ls -la /content/drive/MyDrive/\n    raise FileNotFoundError(\"Dataset not found on Google Drive\")\n\n# Set processed dataset path\nDATASET_PATH = os.path.join(DATASET_ROOT, 'processed')\n\n# Verify structure\nrequired_dirs = ['train', 'val', 'test']\nfor subdir in required_dirs:\n    subdir_path = os.path.join(DATASET_PATH, subdir)\n    if not os.path.exists(subdir_path):\n        raise FileNotFoundError(f\"Expected directory not found: {subdir_path}\")\n    categories = os.listdir(subdir_path)\n    print(f\"‚úÖ {subdir:5s}: {len(categories)} categories\")\n\nprint(f\"\\n‚úÖ Dataset ready: {DATASET_PATH}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "config"
   },
   "source": [
    "# Configuration - OPTIMIZED for Classification\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Training config\n",
    "INPUT_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Categories\n",
    "CATEGORIES = [\n",
    "    'crowd_scene',\n",
    "    'garage_pitlane',\n",
    "    'podium_celebration',\n",
    "    'portrait_paddock',\n",
    "    'racing_action'\n",
    "]\n",
    "NUM_CLASSES = len(CATEGORIES)\n",
    "\n",
    "# ResNet18 OPTIMIZED config\n",
    "CONFIG = {\n",
    "    # Phase 1: Train classification head only\n",
    "    'phase1_epochs': 15,\n",
    "    'phase1_lr': 5e-4,      # Lower than before (was 1e-3)\n",
    "    \n",
    "    # Phase 2: Fine-tune with gradual unfreezing\n",
    "    'phase2_epochs': 35,\n",
    "    'phase2_lr': 1e-4,\n",
    "    'unfreeze_layers': 30,  # ResNet18 has fewer layers\n",
    "    \n",
    "    # Architecture\n",
    "    'dense_units': 256,\n",
    "    'dropout': 0.4,         # Increased from 0.3\n",
    "    \n",
    "    # Regularization\n",
    "    'label_smoothing': 0.1,\n",
    "    'mixup_alpha': 0.2,\n",
    "    \n",
    "    # Early stopping\n",
    "    'patience': 10,         # Increased from 5\n",
    "}\n",
    "\n",
    "# Data augmentation - ENHANCED\n",
    "AUGMENTATION = {\n",
    "    'rotation_range': 25,           # Increased from 15\n",
    "    'width_shift_range': 0.15,      # Increased from 0.1\n",
    "    'height_shift_range': 0.15,     # Increased from 0.1\n",
    "    'zoom_range': 0.2,              # Increased from 0.15\n",
    "    'brightness_range': (0.7, 1.3), # Increased from (0.8, 1.2)\n",
    "    'shear_range': 0.15,            # NEW\n",
    "    'channel_shift_range': 20.0,    # NEW\n",
    "    'horizontal_flip': True,\n",
    "    'fill_mode': 'reflect',         # Changed from 'nearest'\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"\\nüìã Key settings:\")\n",
    "print(f\"   Model: ResNet18 (11.7M params)\")\n",
    "print(f\"   Label smoothing: {CONFIG['label_smoothing']}\")\n",
    "print(f\"   MixUp alpha: {CONFIG['mixup_alpha']}\")\n",
    "print(f\"   Dropout: {CONFIG['dropout']}\")\n",
    "print(f\"   Phase 1 LR: {CONFIG['phase1_lr']}\")\n",
    "print(f\"   Phase 2 LR: {CONFIG['phase2_lr']}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "data_loading"
   },
   "source": "# Data loading with MixUp augmentation\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom typing import Tuple, Dict, Generator\nimport numpy as np\n\ndef compute_class_weights(train_dir: Path) -> Dict[int, float]:\n    \"\"\"Compute optimized class weights for imbalanced datasets\"\"\"\n    class_counts = {}\n    \n    for i, category in enumerate(sorted(CATEGORIES)):\n        category_dir = train_dir / category\n        if category_dir.exists():\n            n_images = len(list(category_dir.glob('*.jpg')))\n            class_counts[i] = n_images\n    \n    total_images = sum(class_counts.values())\n    n_classes = len(class_counts)\n    \n    # Effective number weighting (better for extreme imbalance)\n    beta = (total_images - 1) / total_images\n    class_weights = {\n        i: (1 - beta) / (1 - beta ** count)\n        for i, count in class_counts.items()\n    }\n    \n    # Normalize to prevent huge values\n    max_weight = max(class_weights.values())\n    class_weights = {k: v / max_weight * 2 for k, v in class_weights.items()}\n    \n    return class_weights\n\ndef mixup_generator(generator, alpha=0.2):\n    \"\"\"\n    MixUp augmentation for classification.\n    Blends images and labels for better generalization.\n    \"\"\"\n    while True:\n        # Get two batches\n        x1, y1 = next(generator)\n        x2, y2 = next(generator)\n        \n        # Ensure same batch size\n        batch_size = min(len(x1), len(x2))\n        x1, y1 = x1[:batch_size], y1[:batch_size]\n        x2, y2 = x2[:batch_size], y2[:batch_size]\n        \n        # MixUp: blend images and labels\n        lam = np.random.beta(alpha, alpha, batch_size).reshape(-1, 1, 1, 1)\n        lam_y = lam.reshape(-1, 1)\n        \n        x_mixed = lam * x1 + (1 - lam) * x2\n        y_mixed = lam_y * y1 + (1 - lam_y) * y2\n        \n        yield x_mixed, y_mixed\n\ndef create_data_generators():\n    \"\"\"Create training and validation data generators with ENHANCED augmentation\"\"\"\n    \n    # Get preprocessing function for ResNet - IMPORT CORRETTO\n    from classification_models.keras import Classifiers\n    _, preprocess_input = Classifiers.get('resnet18')\n    \n    # Training augmentation - ENHANCED\n    train_datagen = ImageDataGenerator(\n        preprocessing_function=preprocess_input,\n        rotation_range=AUGMENTATION['rotation_range'],\n        width_shift_range=AUGMENTATION['width_shift_range'],\n        height_shift_range=AUGMENTATION['height_shift_range'],\n        zoom_range=AUGMENTATION['zoom_range'],\n        brightness_range=AUGMENTATION['brightness_range'],\n        shear_range=AUGMENTATION['shear_range'],\n        channel_shift_range=AUGMENTATION['channel_shift_range'],\n        horizontal_flip=AUGMENTATION['horizontal_flip'],\n        fill_mode=AUGMENTATION['fill_mode']\n    )\n    \n    # Validation (only preprocessing, no augmentation)\n    val_datagen = ImageDataGenerator(\n        preprocessing_function=preprocess_input\n    )\n    \n    return train_datagen, val_datagen\n\ndef load_datasets(train_datagen, val_datagen, dataset_path):\n    \"\"\"Load train and validation datasets\"\"\"\n    train_path = Path(dataset_path) / 'train'\n    val_path = Path(dataset_path) / 'val'\n    \n    # Training set\n    train_generator = train_datagen.flow_from_directory(\n        str(train_path),\n        target_size=INPUT_SIZE,\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        shuffle=True,\n        seed=RANDOM_SEED\n    )\n    \n    # Validation set\n    val_generator = val_datagen.flow_from_directory(\n        str(val_path),\n        target_size=INPUT_SIZE,\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        shuffle=False\n    )\n    \n    return train_generator, val_generator\n\n# Load data\ntrain_datagen, val_datagen = create_data_generators()\ntrain_gen, val_gen = load_datasets(train_datagen, val_datagen, DATASET_PATH)\n\n# Compute class weights\nclass_weights = compute_class_weights(Path(DATASET_PATH) / 'train')\n\nprint(f\"\\nüìä Dataset Info:\")\nprint(f\"  Train samples: {train_gen.samples}\")\nprint(f\"  Val samples: {val_gen.samples}\")\nprint(f\"  Classes: {CATEGORIES}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"\\n‚öñÔ∏è  Class weights (normalized):\")\nfor i, cat in enumerate(sorted(CATEGORIES)):\n    print(f\"   {cat}: {class_weights[i]:.3f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "model_builder"
   },
   "source": "# Model building - ResNet18 with optimized head\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom classification_models.keras import Classifiers  # IMPORT CORRETTO\n\ndef build_resnet18_model(num_classes: int, freeze_base: bool = True):\n    \"\"\"\n    Build ResNet18 with optimized classification head.\n    \n    Key improvements:\n    - ResNet18 (11.7M params) instead of ResNet50 (25.6M) - prevents overfitting\n    - BatchNorm in dense head for stability\n    - Higher dropout (0.4) for regularization\n    \"\"\"\n    print(\"\\nüèóÔ∏è  Building ResNet18 Classification Model...\")\n    \n    # Get ResNet18 from classification_models library\n    ResNet18, _ = Classifiers.get('resnet18')\n    \n    base_model = ResNet18(\n        input_shape=(*INPUT_SIZE, 3),\n        weights='imagenet',\n        include_top=False\n    )\n    \n    base_model.trainable = not freeze_base\n    \n    print(f\"  Base model: ResNet18\")\n    print(f\"  Base parameters: {base_model.count_params():,}\")\n    print(f\"  Trainable: {not freeze_base}\")\n    print(f\"  Total base layers: {len(base_model.layers)}\")\n    \n    # Build optimized classification head\n    inputs = keras.Input(shape=(*INPUT_SIZE, 3))\n    \n    # Base model (let Keras handle training mode automatically)\n    x = base_model(inputs)\n    \n    # Global pooling\n    x = layers.GlobalAveragePooling2D(name='global_avg_pool')(x)\n    \n    # Dense block with BatchNorm (stabilizes training)\n    x = layers.Dense(CONFIG['dense_units'], activation=None, name='dense_classifier')(x)\n    x = layers.BatchNormalization(name='bn_dense')(x)\n    x = layers.Activation('relu', name='relu_dense')(x)\n    \n    # Dropout (increased for small dataset)\n    x = layers.Dropout(CONFIG['dropout'], name='dropout')(x)\n    \n    # Output layer\n    outputs = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n    \n    model = keras.Model(inputs, outputs, name='resnet18_scene_classifier')\n    \n    print(f\"\\n  Model architecture:\")\n    print(f\"    Input: {INPUT_SIZE[0]}x{INPUT_SIZE[1]}x3\")\n    print(f\"    Base: ResNet18 (ImageNet pretrained)\")\n    print(f\"    Pool: GlobalAveragePooling2D\")\n    print(f\"    Dense: {CONFIG['dense_units']} units + BatchNorm + ReLU\")\n    print(f\"    Dropout: {CONFIG['dropout']}\")\n    print(f\"    Output: {num_classes} classes (Softmax)\")\n    print(f\"\\n  Total parameters: {model.count_params():,}\")\n    \n    return model, base_model\n\ndef unfreeze_top_layers(base_model, n_layers: int):\n    \"\"\"Unfreeze top N layers of base model for fine-tuning\"\"\"\n    base_model.trainable = True\n    \n    # Freeze all layers except top N\n    for layer in base_model.layers[:-n_layers]:\n        layer.trainable = False\n    \n    trainable_count = sum([1 for layer in base_model.layers if layer.trainable])\n    print(f\"\\nüîì Unfrozen top {trainable_count}/{len(base_model.layers)} layers\")\n\nprint(\"‚úÖ Model builder ready\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "training_utils"
   },
   "source": [
    "# Training utilities with Cosine Annealing LR\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    ReduceLROnPlateau,\n",
    "    LearningRateScheduler,\n",
    "    Callback\n",
    ")\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "def cosine_annealing_schedule(epoch, initial_lr, max_epochs, warmup_epochs=3):\n",
    "    \"\"\"\n",
    "    Cosine annealing learning rate schedule with warmup.\n",
    "    \n",
    "    - Warmup: Linear increase for first N epochs\n",
    "    - Cosine: Smooth decay following cosine curve\n",
    "    \"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        # Linear warmup\n",
    "        return initial_lr * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        # Cosine annealing\n",
    "        progress = (epoch - warmup_epochs) / (max_epochs - warmup_epochs)\n",
    "        return initial_lr * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "def create_callbacks(phase: str):\n",
    "    \"\"\"Create training callbacks with cosine LR scheduling\"\"\"\n",
    "    callbacks = []\n",
    "    \n",
    "    # Model checkpoint\n",
    "    checkpoint_dir = Path('/content/checkpoints/resnet18')\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    checkpoint_cb = ModelCheckpoint(\n",
    "        str(checkpoint_dir / f'{phase}_best.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(checkpoint_cb)\n",
    "    \n",
    "    # Early stopping with increased patience\n",
    "    early_stop_cb = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=CONFIG['patience'],\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(early_stop_cb)\n",
    "    \n",
    "    # Cosine annealing LR schedule\n",
    "    if phase == 'phase1':\n",
    "        max_epochs = CONFIG['phase1_epochs']\n",
    "        initial_lr = CONFIG['phase1_lr']\n",
    "    else:\n",
    "        max_epochs = CONFIG['phase2_epochs']\n",
    "        initial_lr = CONFIG['phase2_lr']\n",
    "    \n",
    "    lr_schedule = LearningRateScheduler(\n",
    "        lambda epoch: cosine_annealing_schedule(epoch, initial_lr, max_epochs),\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(lr_schedule)\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "print(\"‚úÖ Training utilities ready\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_header"
   },
   "source": [
    "## üöÄ Train ResNet18 Scene Classifier\n",
    "\n",
    "**Two-phase transfer learning:**\n",
    "1. **Phase 1**: Train classification head (base frozen)\n",
    "2. **Phase 2**: Fine-tune top layers with lower LR\n",
    "\n",
    "**Key optimizations:**\n",
    "- ResNet18 (vs ResNet50) - prevents overfitting\n",
    "- Label smoothing 0.1\n",
    "- MixUp augmentation\n",
    "- Cosine annealing LR\n",
    "- Class weights for imbalance\n",
    "\n",
    "**Target**: 85-91% validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "train_phase1"
   },
   "source": "# Build model\nmodel, base_model = build_resnet18_model(\n    num_classes=NUM_CLASSES,\n    freeze_base=True\n)\n\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# PHASE 1: Train Classification Head\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(\"\\n\" + \"=\"*60)\nprint(\"üîí PHASE 1: Train Classification Head\")\nprint(\"=\"*60)\n\n# Compile with label smoothing\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=CONFIG['phase1_lr']),\n    loss=keras.losses.CategoricalCrossentropy(label_smoothing=CONFIG['label_smoothing']),\n    metrics=['accuracy']\n)\n\n# Create MixUp generator for training\ntrain_gen_mixup = mixup_generator(train_gen, alpha=CONFIG['mixup_alpha'])\n\n# Calculate steps per epoch\nsteps_per_epoch = train_gen.samples // BATCH_SIZE\n\n# Phase 1 callbacks\nphase1_callbacks = create_callbacks('phase1')\n\n# Train Phase 1 (class_weight rimosso - MixUp + Label Smoothing gestiscono lo sbilanciamento)\nhistory_phase1 = model.fit(\n    train_gen_mixup,\n    steps_per_epoch=steps_per_epoch,\n    validation_data=val_gen,\n    epochs=CONFIG['phase1_epochs'],\n    callbacks=phase1_callbacks,\n    verbose=1\n)\n\nprint(f\"\\n‚úÖ Phase 1 Complete\")\nprint(f\"   Best Val Accuracy: {max(history_phase1.history['val_accuracy']):.4f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "train_phase2"
   },
   "source": "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# PHASE 2: Fine-tune Top Layers\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(\"\\n\" + \"=\"*60)\nprint(\"üîì PHASE 2: Fine-tune Top Layers\")\nprint(\"=\"*60)\n\n# Unfreeze top layers\nunfreeze_top_layers(base_model, CONFIG['unfreeze_layers'])\n\n# Recompile with lower LR for fine-tuning\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=CONFIG['phase2_lr']),\n    loss=keras.losses.CategoricalCrossentropy(label_smoothing=CONFIG['label_smoothing']),\n    metrics=['accuracy']\n)\n\n# Recreate MixUp generator (reset)\ntrain_gen_mixup = mixup_generator(train_gen, alpha=CONFIG['mixup_alpha'])\n\n# Phase 2 callbacks\nphase2_callbacks = create_callbacks('phase2')\n\n# Train Phase 2 (class_weight rimosso - MixUp + Label Smoothing gestiscono lo sbilanciamento)\nhistory_phase2 = model.fit(\n    train_gen_mixup,\n    steps_per_epoch=steps_per_epoch,\n    validation_data=val_gen,\n    epochs=CONFIG['phase2_epochs'],\n    callbacks=phase2_callbacks,\n    verbose=1\n)\n\n# Save final model\nmodel.save('/content/resnet18_scene_classifier_final.keras')\n\nprint(f\"\\n‚úÖ Phase 2 Complete\")\nprint(f\"   Best Val Accuracy: {max(history_phase2.history['val_accuracy']):.4f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "results"
   },
   "source": [
    "# Training results summary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine histories\n",
    "all_accuracy = history_phase1.history['accuracy'] + history_phase2.history['accuracy']\n",
    "all_val_accuracy = history_phase1.history['val_accuracy'] + history_phase2.history['val_accuracy']\n",
    "all_loss = history_phase1.history['loss'] + history_phase2.history['loss']\n",
    "all_val_loss = history_phase1.history['val_loss'] + history_phase2.history['val_loss']\n",
    "\n",
    "# Final metrics\n",
    "final_val_accuracy = max(all_val_accuracy)\n",
    "final_val_loss = min(all_val_loss)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä TRAINING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüèÜ BEST VALIDATION ACCURACY: {final_val_accuracy:.4f}\")\n",
    "print(f\"   Best Validation Loss: {final_val_loss:.4f}\")\n",
    "print(f\"\\n   Phase 1 best: {max(history_phase1.history['val_accuracy']):.4f}\")\n",
    "print(f\"   Phase 2 best: {max(history_phase2.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# Check target\n",
    "target_accuracy = 0.85\n",
    "if final_val_accuracy >= target_accuracy:\n",
    "    print(f\"\\n‚úÖ TARGET REACHED! ({final_val_accuracy:.1%} >= {target_accuracy:.0%})\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Target not reached ({final_val_accuracy:.1%} < {target_accuracy:.0%})\")\n",
    "    print(f\"   Gap: {(target_accuracy - final_val_accuracy)*100:.1f}%\")\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy\n",
    "ax1.plot(all_accuracy, label='Train')\n",
    "ax1.plot(all_val_accuracy, label='Validation')\n",
    "ax1.axvline(x=len(history_phase1.history['accuracy'])-1, color='r', linestyle='--', alpha=0.5, label='Phase 1‚Üí2')\n",
    "ax1.axhline(y=target_accuracy, color='g', linestyle='--', alpha=0.5, label=f'Target ({target_accuracy:.0%})')\n",
    "ax1.set_title('Model Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(all_loss, label='Train')\n",
    "ax2.plot(all_val_loss, label='Validation')\n",
    "ax2.axvline(x=len(history_phase1.history['loss'])-1, color='r', linestyle='--', alpha=0.5, label='Phase 1‚Üí2')\n",
    "ax2.set_title('Model Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Training history saved: /content/training_history.png\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export_header"
   },
   "source": [
    "## üì¶ Export to TensorFlow.js\n",
    "\n",
    "Convert trained model to TensorFlow.js format for Electron deployment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "export_tfjs"
   },
   "source": [
    "import tensorflowjs as tfjs\n",
    "\n",
    "# Load best model\n",
    "best_model = keras.models.load_model('/content/resnet18_scene_classifier_final.keras')\n",
    "\n",
    "# Export to TensorFlow.js\n",
    "tfjs_output_dir = '/content/tfjs_models/resnet18'\n",
    "tfjs.converters.save_keras_model(best_model, tfjs_output_dir)\n",
    "\n",
    "print(f\"\\n‚úÖ Model exported to: {tfjs_output_dir}\")\n",
    "\n",
    "# Export quantized version (int8) for smaller size\n",
    "tfjs_quantized_dir = '/content/tfjs_models/resnet18_quantized'\n",
    "tfjs.converters.save_keras_model(\n",
    "    best_model,\n",
    "    tfjs_quantized_dir,\n",
    "    quantization_dtype_map={'uint8': '*'}\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Quantized model exported to: {tfjs_quantized_dir}\")\n",
    "\n",
    "# Save class labels and config\n",
    "model_info = {\n",
    "    'categories': CATEGORIES,\n",
    "    'category_to_index': {cat: i for i, cat in enumerate(CATEGORIES)},\n",
    "    'index_to_category': {i: cat for i, cat in enumerate(CATEGORIES)},\n",
    "    'input_size': INPUT_SIZE,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'model_type': 'ResNet18',\n",
    "    'preprocessing': 'classification_models resnet18 preprocess_input',\n",
    "    'final_val_accuracy': float(final_val_accuracy),\n",
    "    'training_config': CONFIG\n",
    "}\n",
    "\n",
    "with open(f'{tfjs_output_dir}/model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "with open(f'{tfjs_quantized_dir}/model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "# Create zip for download\n",
    "!zip -r /content/scene_classifier_resnet18_tfjs.zip /content/tfjs_models/\n",
    "\n",
    "print(\"\\n‚úÖ Export complete!\")\n",
    "print(f\"\\nüì¶ Download: /content/scene_classifier_resnet18_tfjs.zip\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "test_model"
   },
   "source": "# Test model on sample images\nfrom tensorflow.keras.preprocessing import image\nfrom classification_models.keras import Classifiers  # IMPORT CORRETTO\nimport numpy as np\n\n# Get preprocessing function\n_, preprocess_input = Classifiers.get('resnet18')\n\n# Load test images\ntest_dir = Path(DATASET_PATH) / 'test'\n\nprint(\"\\nüß™ Testing model on sample images...\\n\")\n\ncorrect = 0\ntotal = 0\n\nfor category in CATEGORIES:\n    category_dir = test_dir / category\n    if not category_dir.exists():\n        continue\n    \n    test_images = list(category_dir.glob('*.jpg'))[:5]  # Test 5 images per category\n    \n    for img_path in test_images:\n        # Load and preprocess image\n        img = image.load_img(img_path, target_size=INPUT_SIZE)\n        img_array = image.img_to_array(img)\n        img_array = preprocess_input(img_array)\n        img_array = np.expand_dims(img_array, axis=0)\n        \n        # Predict\n        predictions = best_model.predict(img_array, verbose=0)\n        predicted_class = CATEGORIES[np.argmax(predictions[0])]\n        confidence = np.max(predictions[0])\n        \n        # Check if correct\n        is_correct = predicted_class == category\n        if is_correct:\n            correct += 1\n        total += 1\n        \n        status = \"‚úÖ\" if is_correct else \"‚ùå\"\n        print(f\"{status} True: {category:20s} | Pred: {predicted_class:20s} | Conf: {confidence:.2%}\")\n\ntest_accuracy = correct / total if total > 0 else 0\nprint(f\"\\nüìä Test Accuracy: {test_accuracy:.2%} ({correct}/{total})\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "download"
   },
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"\\nüì• Downloading trained models...\\n\")\n",
    "\n",
    "# Save training summary\n",
    "training_summary = {\n",
    "    'model': 'ResNet18',\n",
    "    'final_val_accuracy': float(final_val_accuracy),\n",
    "    'final_val_loss': float(final_val_loss),\n",
    "    'phase1_best_acc': float(max(history_phase1.history['val_accuracy'])),\n",
    "    'phase2_best_acc': float(max(history_phase2.history['val_accuracy'])),\n",
    "    'total_epochs': len(all_accuracy),\n",
    "    'parameters': int(model.count_params()),\n",
    "    'config': CONFIG,\n",
    "    'dataset_info': {\n",
    "        'train_samples': train_gen.samples,\n",
    "        'val_samples': val_gen.samples,\n",
    "        'categories': CATEGORIES\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('/content/training_summary.json', 'w') as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "# Download files\n",
    "print(\"1. TensorFlow.js models (all formats)\")\n",
    "files.download('/content/scene_classifier_resnet18_tfjs.zip')\n",
    "\n",
    "print(\"2. Keras model\")\n",
    "files.download('/content/resnet18_scene_classifier_final.keras')\n",
    "\n",
    "print(\"3. Training summary\")\n",
    "files.download('/content/training_summary.json')\n",
    "\n",
    "print(\"4. Training history plot\")\n",
    "files.download('/content/training_history.png')\n",
    "\n",
    "print(\"\\n‚úÖ All downloads complete!\")\n",
    "print(\"\\nüìã Next steps:\")\n",
    "print(\"1. Extract scene_classifier_resnet18_tfjs.zip\")\n",
    "print(\"2. Copy resnet18_quantized/ to RaceTagger project\")\n",
    "print(\"3. Test inference in Electron app\")\n",
    "print(\"4. Integrate preprocessing (use classification_models preprocess_input)\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}